{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Lab Assignment: Predictive Analytics with Text, Sentiment Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "In this lab, you will explore text mining techniques, perform sentiment analysis, and apply topic modeling and named entity recognition to textual data. The lab focuses on text preprocessing, using text representation methods, and extracting insights from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Installations\n",
    "**Objective:** Ensure all necessary packages are installed and imported for the lab.\n",
    "\n",
    "**Tasks:**\n",
    "1. Install required Python packages: Scikit-learn, Pandas, Numpy, Matplotlib, Seaborn, NLTK, and Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\program files\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (0.13.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\program files\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\program files\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\program files\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\w200jrs\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "%pip install scikit-learn pandas numpy matplotlib seaborn nltk gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Libraries\n",
    "**Objective:** Import all necessary libraries for data manipulation, text processing, modeling, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\w200jrs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('pos_tag')\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Explore Text Data\n",
    "**Objective:** Gain a preliminary understanding of the text dataset to be used for analysis.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Load the Dataset:** Import the dataset into a Pandas DataFrame.\n",
    "2. **Inspect the Data:** Use Pandas functions to inspect the first few rows, check for missing values, and understand the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  The product quality is outstanding and I'm ver...  positive\n",
      "1  I had a terrible experience with customer serv...  negative\n",
      "2  Delivery was fast, and the items were well-pac...  positive\n",
      "3  The website is difficult to navigate and confu...  negative\n",
      "4  I'm impressed with the variety of products ava...  positive\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('text_data.csv')\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text Preprocessing\n",
    "**Objective:** Prepare the text data by cleaning and tokenizing it.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Tokenization:** Split text into individual words or tokens.\n",
    "2. **Stop Words Removal:** Use NLTK to remove common stop words from the text.\n",
    "3. **Lemmatization:** Reduce words to their root form using WordNetLemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label'], dtype='object')\n",
      "text\n",
      "<class 'str'>    20\n",
      "Name: count, dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [text, label]\n",
      "Index: []\n",
      "0     product quality outstanding happy purchase\n",
      "1    terrible experience customer service wo buy\n",
      "2                             delivery fast item\n",
      "3           website difficult navigate confusing\n",
      "4            impressed variety product available\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Check if the input is a valid string\n",
    "    if isinstance(text, str):\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text.lower())  \n",
    "        # Lemmatize and remove stopwords\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and t.isalpha()]  \n",
    "        return ' '.join(tokens)  # Join the tokens back into a string\n",
    "    else:\n",
    "        return ''  # Return an empty string for non-string inputs\n",
    "\n",
    "# Ensure 'text' column exists\n",
    "if 'text' in df.columns:\n",
    "    # Check if 'text' column has non-string values\n",
    "    print(df['text'].apply(lambda x: type(x)).value_counts())\n",
    "\n",
    "    # Fill missing values with an empty string\n",
    "    df['text'] = df['text'].fillna('')\n",
    "\n",
    "    print(df[df['text'].isnull()])\n",
    "\n",
    "    # Apply the preprocessing using a list comprehension\n",
    "    cleaned_texts = [preprocess_text(text) for text in df['text']]\n",
    "\n",
    "    # Assign the cleaned texts to a new column in the dataframe\n",
    "    df['cleaned_text'] = cleaned_texts\n",
    "    print(df['cleaned_text'].head())\n",
    "else:\n",
    "    print(\"Column 'text' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sentiment Analysis\n",
    "**Objective:** Build and evaluate a model to classify text based on sentiment (positive, negative).\n",
    "\n",
    "**Tasks:**\n",
    "1. **Vectorization:** Convert text into numerical features using TF-IDF.\n",
    "2. **Model Training:** Train a Logistic Regression model on the text data.\n",
    "3. **Evaluation:** Evaluate the model using accuracy and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (14, 76)\n",
      "Test set size: (6, 76)\n",
      "Logistic Regression Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         4\n",
      "    positive       0.33      1.00      0.50         2\n",
      "\n",
      "    accuracy                           0.33         6\n",
      "   macro avg       0.17      0.50      0.25         6\n",
      "weighted avg       0.11      0.33      0.17         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\w200jrs\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\w200jrs\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\w200jrs\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(df['cleaned_text']).toarray()\n",
    "y = df['label']  # Assuming 'label' is the target column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(f'Training set size: {X_train.shape}')\n",
    "print(f'Test set size: {X_test.shape}')\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Logistic Regression Classification Report')\n",
    "print(classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Topic Modeling with LDA\n",
    "**Objective:** Identify topics within a set of documents using Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "**Tasks:**\n",
    "1. **Prepare Text Data:** Tokenize and remove stop words.\n",
    "2. **Create Dictionary and Corpus:** Use Gensim to create a dictionary and corpus for LDA.\n",
    "3. **Train LDA Model:** Use LDA to find topics within the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.068*\"delivery\" + 0.037*\"experience\" + 0.037*\"support\" + 0.037*\"service\"')\n",
      "(1, '0.046*\"product\" + 0.025*\"need\" + 0.025*\"constant\" + 0.025*\"battery\"')\n",
      "(2, '0.035*\"service\" + 0.035*\"fast\" + 0.035*\"customer\" + 0.035*\"user\"')\n",
      "(3, '0.071*\"quality\" + 0.038*\"regret\" + 0.038*\"good\" + 0.038*\"advertised\"')\n",
      "(4, '0.063*\"use\" + 0.062*\"app\" + 0.062*\"easy\" + 0.062*\"intuitive\"')\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and stop words removal for LDA\n",
    "texts = [text.split() for text in df['cleaned_text']]\n",
    "\n",
    "# Create Dictionary and Corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train LDA Model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Display Topics\n",
    "topics = lda_model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Named Entity Recognition (NER)\n",
    "**Objective:** Extract named entities (e.g., names, dates, locations) from text.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Use NLTK for NER:** Tokenize text and apply NER using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\w200jrs/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\w200jrs\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Tokenize and perform POS tagging\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(example_text)\n\u001b[1;32m----> 6\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Perform Named Entity Recognition\u001b[39;00m\n\u001b[0;32m      9\u001b[0m named_entities \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mne_chunk(pos_tags)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\w200jrs/nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Program Files\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\w200jrs\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Example text for NER\n",
    "example_text = \"Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.\"\n",
    "\n",
    "# Tokenize and perform POS tagging\n",
    "tokens = nltk.word_tokenize(example_text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Perform Named Entity Recognition\n",
    "named_entities = nltk.ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Summary and Discussion\n",
    "**Objective:** Reflect on the use of text mining techniques and discuss their implications in the context of business applications.\n",
    "\n",
    "**Tasks:**\n",
    "1. **Compare Techniques:** Discuss the results from sentiment analysis, topic modeling, and named entity recognition.\n",
    "2. **Business Implications:** Describe how text mining can provide valuable insights for businesses, such as understanding customer feedback and monitoring social media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Submission\n",
    "**Deliverables:**\n",
    "- Jupyter Notebook (.ipynb) with all code, visualizations, and analysis.\n",
    "- A brief report (1-2 paragraphs) summarizing the findings, including sentiment analysis results, topic modeling insights, and NER outcomes.\n",
    "\n",
    "**Deadline:** Submit your completed notebook and report to the course portal by the end of class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
